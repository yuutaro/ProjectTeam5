{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sfsdfsdおれ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ximihu/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ximihu/.local/lib/python3.10/site-packages (4.67.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ximihu/.local/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install tqdm\n",
    "%pip install re\n",
    "%pip install sklearn\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125/1400195009.py:9: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_b = pd.read_csv(data_b_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# データAとデータBのCSVファイルのパス\n",
    "data_a_path = '../data/original_data/race_table/combined.csv'  # データAのCSVファイルパス\n",
    "data_b_path = '../data/merged_data/merged_sorted.csv'  # データBのCSVファイルパス\n",
    "\n",
    "# CSVファイルからデータを読み込む\n",
    "data_a = pd.read_csv(data_a_path,low_memory=False)\n",
    "data_b = pd.read_csv(data_b_path)\n",
    "data_a=data_a.dropna()\n",
    "data_b=data_b.dropna()\n",
    "data_a=data_a.drop(columns=['first_horses', 'second_horses', 'third_horses', 'Time', 'Jockey', 'Trainer'])\n",
    "# Code列でデータを結合 (data_bにdata_aの情報を追加)\n",
    "merged_data = pd.merge(data_b, data_a, on='Code', how='left')\n",
    "\n",
    "#結果の確認\n",
    "merged_data.to_csv('../notebook/data-shin/merged_data_final.csv', index=False)\n",
    "for i in merged_data.columns:\n",
    "    # 各列のユニークな値とその出現回数を取得\n",
    "    value_counts = df[i].value_counts()  # 各値とそのカウント\n",
    "    print(f\"{i}: {value_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量の析出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125/1133966646.py:7: DtypeWarning: Columns (1,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = '../notebook/data-shin/merged_data_final.csv'\n",
    "# CSVファイルの読み込み\n",
    "df = pd.read_csv(file_path)\n",
    "# 12桁のコードのみを抽出\n",
    "df = df[df['Code'].astype(str).str.match(r'^\\d{12}$')]\n",
    "\n",
    "# 正規表現の事前コンパイル（前後にある任意の空白を無視）\n",
    "time_pattern1 = re.compile(r'\\s*(\\d+):(\\d+\\.\\d+)\\s*')\n",
    "time_pattern2 = re.compile(r'\\s*(\\d+):(\\d+):(\\d+)\\s*')\n",
    "sex_age_pattern = re.compile(r'\\s*([牡牝セ])(\\d+)\\s*')\n",
    "distance_pattern = re.compile(r'\\s*([ダ芝障])(\\d+)\\s*')\n",
    "condition_pattern = re.compile(r'\\s*([良重稍不])\\s*')\n",
    "weather_pattern = re.compile(r'\\s*(晴|曇|小雨|雨|雪|小雪)\\s*')\n",
    "kinryou_pattern = re.compile(r'(\\d+)\\(([\\+\\-]?\\d+)\\)')\n",
    "\n",
    "\n",
    "# 時間を秒に変換する関数 (ベクトル化)\n",
    "def time_to_seconds(time_series):\n",
    "    result = []\n",
    "    for time_str in time_series:\n",
    "        if isinstance(time_str, str):\n",
    "            match1 = time_pattern1.match(time_str)\n",
    "            if match1:\n",
    "                minutes, seconds = int(match1.group(1)), float(match1.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "            match2 = time_pattern2.match(time_str)\n",
    "            if match2:\n",
    "                minutes, seconds = int(match2.group(1)), float(match2.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "        result.append(None)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 性別と年齢を抽出する関数（ベクトル化）\n",
    "def extract_sex_age_column(sex_age_series):\n",
    "    sexes, ages = [], []\n",
    "    for text in sex_age_series:\n",
    "        match = sex_age_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            sex = match.group(1)\n",
    "            if sex in ['牡', '牝', 'セ']:  # 性別が牡、牝、セのみを許可\n",
    "                sexes.append(sex)\n",
    "                ages.append(int(match.group(2)))\n",
    "            else:\n",
    "                sexes.append(None)\n",
    "                ages.append(None)\n",
    "        else:\n",
    "            sexes.append(None)\n",
    "            ages.append(None)\n",
    "    return pd.DataFrame({'Sex': sexes, 'Age': ages})\n",
    "\n",
    "\n",
    "# 距離を抽出する関数（ベクトル化）\n",
    "def extract_distance_column(distance_series):\n",
    "    statuses, distances = [], []\n",
    "    for text in distance_series:\n",
    "        match = distance_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            statuses.append(match.group(1))\n",
    "            distances.append(int(match.group(2)))\n",
    "        else:\n",
    "            statuses.append(None)\n",
    "            distances.append(None)\n",
    "    return pd.DataFrame({'Sta': statuses, 'Dis': distances})\n",
    "\n",
    "\n",
    "# 条件を抽出する関数（ベクトル化）\n",
    "def extract_condition_column(condition_series):\n",
    "    conditions = []\n",
    "    \n",
    "    for text in condition_series:\n",
    "        match = condition_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            conditions.append(match.group(1))\n",
    "        else:\n",
    "            conditions.append(None)\n",
    "    return pd.Series(conditions)\n",
    "\n",
    "\n",
    "# 天気を抽出する関数（ベクトル化）\n",
    "def extract_weather_column(weather_series):\n",
    "    weathers = []\n",
    "    for text in weather_series:\n",
    "        match = weather_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weathers.append(match.group(1))\n",
    "        else:\n",
    "            weathers.append(None)\n",
    "    return pd.Series(weathers)\n",
    "\n",
    "\n",
    "# 体重の変化を抽出する関数\n",
    "def extract_weight_change(weight_series):\n",
    "    weights, changes = [], []\n",
    "    for text in weight_series:\n",
    "        match = re.match(r'(\\d+)\\(([\\+\\-]?\\d+)\\)', text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weights.append(int(match.group(1)))  # 体重\n",
    "            changes.append(int(match.group(2)))  # 変化量\n",
    "        else:\n",
    "            weights.append(None)\n",
    "            changes.append(None)\n",
    "    return pd.DataFrame({'Weight': weights, 'Change': changes})\n",
    "\n",
    "\n",
    "\n",
    "# tqdmの進捗バーを使用して変換\n",
    "tqdm.pandas()\n",
    "df['Time'] = time_to_seconds(df['Time'].values)\n",
    "\n",
    "# SexとAgeの列に分割して格納\n",
    "sex_age_df = extract_sex_age_column(df['Sex/Age'].values)\n",
    "df[['Sex', 'Age']] = sex_age_df\n",
    "\n",
    "# StaとDisの列に分割して格納\n",
    "distance_df = extract_distance_column(df['Distance'].values)\n",
    "df[['Sta', 'Dis']] = distance_df\n",
    "\n",
    "# Conditionの列に分割して格納\n",
    "df['Condition'] = extract_condition_column(df['Condition'].values)\n",
    "\n",
    "# Weatherの列に分割して格納\n",
    "df['Weather'] = extract_weather_column(df['Weather'].values)\n",
    "# 新しいカラム 'Horse Weight' を追加して処理\n",
    "weight_change_df = extract_weight_change(df['Horse Weight'].values)\n",
    "df[['Weight', 'Change']] = weight_change_df\n",
    "\n",
    "# NaN 値が含まれている行をフィルタリング\n",
    "df = df.dropna(subset=['Sex', 'Age', 'Sta', 'Dis', 'Condition', 'Weather'])\n",
    "\n",
    "# 不要な列を削除\n",
    "df = df.drop('Sex/Age', axis=1)\n",
    "df = df.drop('Distance', axis=1)\n",
    "df=df.drop('Horse Weight',axis=1)\n",
    "# フィルタリングしたデータをCSVに保存\n",
    "df.to_csv('../notebook/data-shin/filtered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40468/1983141988.py:5: DtypeWarning: Columns (1,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../notebook/data-shin/filtered_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Frame Rank  Horse Name   Kinryou    Jockey      Time    Nobori  \\\n",
      "Frame Rank     1.000000   -0.011883 -0.000154 -0.005887  0.000284  0.019357   \n",
      "Horse Name    -0.011883    1.000000 -0.132046  0.619328  0.075380 -0.067430   \n",
      "Kinryou       -0.000154   -0.132046  1.000000 -0.078536  0.309331 -0.310887   \n",
      "Jockey        -0.005887    0.619328 -0.078536  1.000000  0.061148 -0.035005   \n",
      "Time           0.000284    0.075380  0.309331  0.061148  1.000000 -0.360421   \n",
      "Nobori         0.019357   -0.067430 -0.310887 -0.035005 -0.360421  1.000000   \n",
      "Ninki          0.001498    0.052998 -0.061918  0.074946  0.024220 -0.045414   \n",
      "Trainer       -0.003077    0.618055 -0.102605  0.525383  0.059373 -0.051512   \n",
      "Banushi       -0.004301    0.593453 -0.110018  0.447008  0.022206  0.019182   \n",
      "Date           0.012155   -0.943088  0.134428 -0.673825 -0.082432  0.069828   \n",
      "Track         -0.023983    0.305238  0.038204  0.229157  0.133146 -0.286749   \n",
      "Weather       -0.002136   -0.011594 -0.009573 -0.017670 -0.018849  0.022734   \n",
      "Race Number    0.009479   -0.007677  0.212467 -0.011571  0.160729 -0.092336   \n",
      "Sex            0.003178   -0.044771 -0.632052 -0.037548 -0.174348  0.084113   \n",
      "Age            0.010645    0.018106  0.245616  0.018693  0.122383 -0.010470   \n",
      "Dis           -0.003897    0.116455  0.312394  0.081831  0.982450 -0.423208   \n",
      "Weight         0.002836   -0.033740  0.361406 -0.049894  0.127860 -0.075419   \n",
      "Change         0.000948   -0.020446  0.009048 -0.014683 -0.003500 -0.010659   \n",
      "\n",
      "                Ninki   Trainer   Banushi      Date     Track   Weather  \\\n",
      "Frame Rank   0.001498 -0.003077 -0.004301  0.012155 -0.023983 -0.002136   \n",
      "Horse Name   0.052998  0.618055  0.593453 -0.943088  0.305238 -0.011594   \n",
      "Kinryou     -0.061918 -0.102605 -0.110018  0.134428  0.038204 -0.009573   \n",
      "Jockey       0.074946  0.525383  0.447008 -0.673825  0.229157 -0.017670   \n",
      "Time         0.024220  0.059373  0.022206 -0.082432  0.133146 -0.018849   \n",
      "Nobori      -0.045414 -0.051512  0.019182  0.069828 -0.286749  0.022734   \n",
      "Ninki        1.000000  0.071353  0.044213 -0.048997  0.061966 -0.002605   \n",
      "Trainer      0.071353  1.000000  0.460391 -0.665183  0.273975 -0.027309   \n",
      "Banushi      0.044213  0.460391  1.000000 -0.622808  0.165902 -0.002810   \n",
      "Date        -0.048997 -0.665183 -0.622808  1.000000 -0.340356  0.013593   \n",
      "Track        0.061966  0.273975  0.165902 -0.340356  1.000000 -0.050927   \n",
      "Weather     -0.002605 -0.027309 -0.002810  0.013593 -0.050927  1.000000   \n",
      "Race Number  0.054874 -0.011328 -0.016995  0.000074  0.026123  0.007955   \n",
      "Sex          0.031336 -0.041130 -0.043531  0.073500 -0.063640  0.007184   \n",
      "Age          0.067588 -0.005524  0.036567  0.012099 -0.134274  0.015965   \n",
      "Dis          0.039684  0.087379  0.036009 -0.132652  0.210070 -0.021545   \n",
      "Weight      -0.089624 -0.046196 -0.035855  0.042286  0.022382 -0.008228   \n",
      "Change      -0.018826 -0.014005 -0.015622  0.020329  0.001962  0.005027   \n",
      "\n",
      "             Race Number       Sex       Age       Dis    Weight    Change  \n",
      "Frame Rank      0.009479  0.003178  0.010645 -0.003897  0.002836  0.000948  \n",
      "Horse Name     -0.007677 -0.044771  0.018106  0.116455 -0.033740 -0.020446  \n",
      "Kinryou         0.212467 -0.632052  0.245616  0.312394  0.361406  0.009048  \n",
      "Jockey         -0.011571 -0.037548  0.018693  0.081831 -0.049894 -0.014683  \n",
      "Time            0.160729 -0.174348  0.122383  0.982450  0.127860 -0.003500  \n",
      "Nobori         -0.092336  0.084113 -0.010470 -0.423208 -0.075419 -0.010659  \n",
      "Ninki           0.054874  0.031336  0.067588  0.039684 -0.089624 -0.018826  \n",
      "Trainer        -0.011328 -0.041130 -0.005524  0.087379 -0.046196 -0.014005  \n",
      "Banushi        -0.016995 -0.043531  0.036567  0.036009 -0.035855 -0.015622  \n",
      "Date            0.000074  0.073500  0.012099 -0.132652  0.042286  0.020329  \n",
      "Track           0.026123 -0.063640 -0.134274  0.210070  0.022382  0.001962  \n",
      "Weather         0.007955  0.007184  0.015965 -0.021545 -0.008228  0.005027  \n",
      "Race Number     1.000000 -0.126118  0.314575  0.209540  0.207400  0.000576  \n",
      "Sex            -0.126118  1.000000 -0.109268 -0.190079 -0.418113 -0.003544  \n",
      "Age             0.314575 -0.109268  1.000000  0.099554  0.201045 -0.018160  \n",
      "Dis             0.209540 -0.190079  0.099554  1.000000  0.145754 -0.002672  \n",
      "Weight          0.207400 -0.418113  0.201045  0.145754  1.000000  0.089530  \n",
      "Change          0.000576 -0.003544 -0.018160 -0.002672  0.089530  1.000000  \n",
      "Top 10 correlated pairs:\n",
      "1. Time - Dis: 0.982\n",
      "2. Horse Name - Date: 0.943\n",
      "3. Jockey - Date: 0.674\n",
      "4. Trainer - Date: 0.665\n",
      "5. Kinryou - Sex: 0.632\n",
      "6. Banushi - Date: 0.623\n",
      "7. Horse Name - Jockey: 0.619\n",
      "8. Horse Name - Trainer: 0.618\n",
      "9. Horse Name - Banushi: 0.593\n",
      "10. Jockey - Trainer: 0.525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv')\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "# 方法1: 文字列カテゴリを整数に変換\n",
    "df['Horse Name'] = pd.factorize(df['Horse Name'])[0]\n",
    "df['Jockey'] = pd.factorize(df['Jockey'])[0]\n",
    "df['Trainer'] = pd.factorize(df['Trainer'])[0]\n",
    "df['Banushi'] = pd.factorize(df['Banushi'])[0]\n",
    "df['Track'] = pd.factorize(df['Track'])[0]\n",
    "df['Weather'] = pd.factorize(df['Weather'])[0]\n",
    "df['Sex'] = pd.factorize(df['Sex'])[0]\n",
    "\n",
    "# 方法2: 日付データをカテゴリ型に変換する例\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())  # 日付を数値化（連続した数値）\n",
    "\n",
    "#Codeカラムを消す\n",
    "df=df.drop('Code',axis=1)\n",
    "df=df.drop('Horse Number',axis=1)\n",
    "\n",
    "# Weight列の処理 - '5(降)'のような文字列から数値のみを抽出\n",
    "df['Weight'] = df['Weight'].apply(lambda x: float(str(x).split('(')[0]) if pd.notnull(x) else np.nan)\n",
    "df['Change'] = df['Change'].apply(lambda x: float(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 数値型の列のみを選択して相関係数行列を計算\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "print(corr_matrix)\n",
    "# 相関係数の絶対値を取得し、上位10ペアを抽出（対角要素を除く）\n",
    "corr_pairs = []\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        corr_pairs.append({\n",
    "            'pair': (numeric_cols[i], numeric_cols[j]),\n",
    "            'correlation': abs(corr_matrix.iloc[i,j])\n",
    "        })\n",
    "\n",
    "# 相関係数の絶対値で降順ソート\n",
    "corr_pairs = sorted(corr_pairs, key=lambda x: x['correlation'], reverse=True)\n",
    "\n",
    "# 上位10ペアを表示\n",
    "print(\"Top 10 correlated pairs:\")\n",
    "for i, pair in enumerate(corr_pairs[:10], 1):\n",
    "    print(f\"{i}. {pair['pair'][0]} - {pair['pair'][1]}: {pair['correlation']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.5553754562406237\n",
      "Mean Absolute Error: 0.9086452096877902\n",
      "R2 Score: 0.9962767496176523\n",
      "       Actual Time  Predicted Time\n",
      "14558         75.5          74.344\n",
      "47286         85.0          84.635\n",
      "28091         89.7          90.865\n",
      "29606         76.0          75.735\n",
      "54587         91.9          91.940\n",
      "44941        112.6         109.795\n",
      "58314         82.2          82.617\n",
      "34716        112.6         111.997\n",
      "39026        104.2         104.913\n",
      "13444        120.8         120.623\n",
      "       Actual Time  Predicted Time\n",
      "5755         118.4         118.585\n",
      "22825         93.0          92.837\n",
      "11186         88.5          88.651\n",
      "31937        107.6         106.935\n",
      "68286         73.8          73.315\n",
      "52256         81.0          81.832\n",
      "14015         96.0          95.076\n",
      "60488         94.9          96.594\n",
      "3622          92.1          92.799\n",
      "19656         69.9          69.921\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# データの読み込み\n",
    "try:\n",
    "    df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=70000)\n",
    "except Exception as e:\n",
    "    print(f\"Error while reading the data: {e}\")\n",
    "\n",
    "# 必要なカテゴリ変数にOne-Hotエンコーディングを適用\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "try:\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error while applying One-Hot Encoding: {e}\")\n",
    "\n",
    "# 日付データを数値化\n",
    "try:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "except Exception as e:\n",
    "    print(f\"Error while converting date to ordinal: {e}\")\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "try:\n",
    "    df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "except Exception as e:\n",
    "    print(f\"Error while processing Weight column: {e}\")\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "try:\n",
    "    df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "except Exception as e:\n",
    "    print(f\"Error while processing Change column: {e}\")\n",
    "\n",
    "# 欠損値を含む行を削除（補完方法も検討）\n",
    "try:\n",
    "    df = df.dropna()  # もしデータが不足している場合、欠損値の補完も考慮する\n",
    "except Exception as e:\n",
    "    print(f\"Error while dropping rows with missing values: {e}\")\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code', 'Horse Number'])\n",
    "\n",
    "# 確保するために、説明変数はすべて数値型に\n",
    "try:\n",
    "    features = features.select_dtypes(include=[np.number])\n",
    "except Exception as e:\n",
    "    print(f\"Error while selecting numeric features: {e}\")\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "except Exception as e:\n",
    "    print(f\"Error while splitting the data into training and testing sets: {e}\")\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 予測結果と実際のタイムの差を計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    # 予測タイムと実際のタイムを表示\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    \n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習でタイム予測(500000行)(ランダムフォレスト)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9647742792619719\n",
      "Mean Absolute Error: 0.7408475315683096\n",
      "R2 Score: 0.9975617383110674\n",
      "        Actual Time  Predicted Time\n",
      "440484         93.2          92.290\n",
      "170130         75.4          76.226\n",
      "84159          75.8          75.233\n",
      "272923         84.3          85.704\n",
      "195556         88.7          88.479\n",
      "265601         98.1          97.176\n",
      "63784          99.1          99.643\n",
      "485157        103.8         105.160\n",
      "385200         69.0          69.347\n",
      "14127          78.2          77.399\n",
      "        Actual Time  Predicted Time\n",
      "58116          93.8          93.299\n",
      "186338         93.5          94.570\n",
      "257155         91.3          91.065\n",
      "336424         92.4          93.133\n",
      "30412         114.3         113.148\n",
      "116819        116.2         116.077\n",
      "459483         93.5          92.921\n",
      "432530         85.7          87.369\n",
      "299024         78.5          79.309\n",
      "158697         77.5          77.423\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=500000)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code','Horse Number'])\n",
    "\n",
    "# 确保所有的特征列都是数值型\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 予測結果と実際のタイムの差を計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    # 予測タイムと実際のタイムを表示\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    \n",
    "    \n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU加速on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[39 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/requirements.py\", line 36, in __init__\n",
      "  \u001b[31m   \u001b[0m     parsed = _parse_requirement(requirement_string)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 62, in parse_requirement\n",
      "  \u001b[31m   \u001b[0m     return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 80, in _parse_requirement\n",
      "  \u001b[31m   \u001b[0m     url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 124, in _parse_requirement_details\n",
      "  \u001b[31m   \u001b[0m     marker = _parse_requirement_marker(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 145, in _parse_requirement_marker\n",
      "  \u001b[31m   \u001b[0m     tokenizer.raise_syntax_error(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_tokenizer.py\", line 167, in raise_syntax_error\n",
      "  \u001b[31m   \u001b[0m     raise ParserSyntaxError(\n",
      "  \u001b[31m   \u001b[0m packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-9xe96ht1/tensorflow-gpu_6e220518616842a49a60ab31bd6ec4cb/setup.py\", line 40, in <module>\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/__init__.py\", line 116, in setup\n",
      "  \u001b[31m   \u001b[0m     _install_setup_requires(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/__init__.py\", line 87, in _install_setup_requires\n",
      "  \u001b[31m   \u001b[0m     dist.parse_config_files(ignore_option_errors=True)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/dist.py\", line 657, in parse_config_files\n",
      "  \u001b[31m   \u001b[0m     self._finalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/dist.py\", line 387, in _finalize_requires\n",
      "  \u001b[31m   \u001b[0m     self._normalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/dist.py\", line 402, in _normalize_requires\n",
      "  \u001b[31m   \u001b[0m     self.install_requires = list(map(str, _reqs.parse(install_requires)))\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/requirements.py\", line 38, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise InvalidRequirement(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ximihu/.local/lib/python3.10/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ximihu/.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ximihu/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-gpu\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=500000)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code','Horse Number'])\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    # モデルの訓練\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 評価指標の計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "\n",
    "    # クロスバリデーションの実施（MAEを評価）\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    print(f\"Cross-Validation MAE: {-np.mean(cv_scores)}\")\n",
    "\n",
    "    # 実際のタイムと予測タイムの比較\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "\n",
    "    # 実際のタイムと予測タイムの散布図\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel(\"Actual Time\")\n",
    "    plt.ylabel(\"Predicted Time\")\n",
    "    plt.title(\"Actual vs Predicted Time\")\n",
    "    plt.show()\n",
    "\n",
    "    # 残差の分布をヒストグラムで可視化\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=50, alpha=0.7)\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.920403952772265\n",
      "Mean Absolute Error: 1.0240662618078646\n",
      "R2 Score: 0.9957350893126796\n",
      "         Actual Time  Predicted Time\n",
      "831573          99.2            97.1\n",
      "752164         122.8           122.6\n",
      "1166192        107.7           108.6\n",
      "899728          94.0            93.3\n",
      "863278          90.2            90.6\n",
      "483162          93.2            94.1\n",
      "839082          74.6            74.8\n",
      "292536         145.6           145.4\n",
      "821774          89.7            90.1\n",
      "557207          87.0            88.2\n",
      "         Actual Time  Predicted Time\n",
      "873105          88.9            88.2\n",
      "899451          61.9            60.2\n",
      "124218          79.1            79.3\n",
      "662252          54.9            54.8\n",
      "253838          99.0            98.4\n",
      "199580          97.3            98.6\n",
      "1271776         71.4            72.5\n",
      "975687         106.5           109.4\n",
      "775385          89.2            87.4\n",
      "908827          91.9            92.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code','Horse Number'])\n",
    "\n",
    "# 确保所有的特征列都是数値型\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "#  線形回帰 (Linear Regression)\n",
    "model_linear = LinearRegression()\n",
    "# 決定木回帰 (Decision Tree Regressor)\n",
    "model_tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# 使用するモデルを選択\n",
    "model = model_tree  # ここで他のモデル（model_tree）に変更できます\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 予測結果と実際のタイムの差を計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    # 予測タイムと実際のタイムを表示\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    \n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
