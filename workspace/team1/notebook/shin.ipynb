{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sfsdfsdおれ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ximihu/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ximihu/.local/lib/python3.10/site-packages (4.67.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ximihu/.local/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install tqdm\n",
    "%pip install re\n",
    "%pip install sklearn\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125/1400195009.py:9: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_b = pd.read_csv(data_b_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# データAとデータBのCSVファイルのパス\n",
    "data_a_path = '../data/original_data/race_table/combined.csv'  # データAのCSVファイルパス\n",
    "data_b_path = '../data/merged_data/merged_sorted.csv'  # データBのCSVファイルパス\n",
    "\n",
    "# CSVファイルからデータを読み込む\n",
    "data_a = pd.read_csv(data_a_path,low_memory=False)\n",
    "data_b = pd.read_csv(data_b_path)\n",
    "data_a=data_a.dropna()\n",
    "data_b=data_b.dropna()\n",
    "data_a=data_a.drop(columns=['first_horses', 'second_horses', 'third_horses', 'Time', 'Jockey', 'Trainer'])\n",
    "# Code列でデータを結合 (data_bにdata_aの情報を追加)\n",
    "merged_data = pd.merge(data_b, data_a, on='Code', how='left')\n",
    "\n",
    "#結果の確認\n",
    "merged_data.to_csv('../notebook/data-shin/merged_data_final.csv', index=False)\n",
    "for i in merged_data.columns:\n",
    "    # 各列のユニークな値とその出現回数を取得\n",
    "    value_counts = df[i].value_counts()  # 各値とそのカウント\n",
    "    print(f\"{i}: {value_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量の析出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125/1133966646.py:7: DtypeWarning: Columns (1,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = '../notebook/data-shin/merged_data_final.csv'\n",
    "# CSVファイルの読み込み\n",
    "df = pd.read_csv(file_path)\n",
    "# 12桁のコードのみを抽出\n",
    "df = df[df['Code'].astype(str).str.match(r'^\\d{12}$')]\n",
    "\n",
    "# 正規表現の事前コンパイル（前後にある任意の空白を無視）\n",
    "time_pattern1 = re.compile(r'\\s*(\\d+):(\\d+\\.\\d+)\\s*')\n",
    "time_pattern2 = re.compile(r'\\s*(\\d+):(\\d+):(\\d+)\\s*')\n",
    "sex_age_pattern = re.compile(r'\\s*([牡牝セ])(\\d+)\\s*')\n",
    "distance_pattern = re.compile(r'\\s*([ダ芝障])(\\d+)\\s*')\n",
    "condition_pattern = re.compile(r'\\s*([良重稍不])\\s*')\n",
    "weather_pattern = re.compile(r'\\s*(晴|曇|小雨|雨|雪|小雪)\\s*')\n",
    "kinryou_pattern = re.compile(r'(\\d+)\\(([\\+\\-]?\\d+)\\)')\n",
    "\n",
    "\n",
    "# 時間を秒に変換する関数 (ベクトル化)\n",
    "def time_to_seconds(time_series):\n",
    "    result = []\n",
    "    for time_str in time_series:\n",
    "        if isinstance(time_str, str):\n",
    "            match1 = time_pattern1.match(time_str)\n",
    "            if match1:\n",
    "                minutes, seconds = int(match1.group(1)), float(match1.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "            match2 = time_pattern2.match(time_str)\n",
    "            if match2:\n",
    "                minutes, seconds = int(match2.group(1)), float(match2.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "        result.append(None)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 性別と年齢を抽出する関数（ベクトル化）\n",
    "def extract_sex_age_column(sex_age_series):\n",
    "    sexes, ages = [], []\n",
    "    for text in sex_age_series:\n",
    "        match = sex_age_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            sex = match.group(1)\n",
    "            if sex in ['牡', '牝', 'セ']:  # 性別が牡、牝、セのみを許可\n",
    "                sexes.append(sex)\n",
    "                ages.append(int(match.group(2)))\n",
    "            else:\n",
    "                sexes.append(None)\n",
    "                ages.append(None)\n",
    "        else:\n",
    "            sexes.append(None)\n",
    "            ages.append(None)\n",
    "    return pd.DataFrame({'Sex': sexes, 'Age': ages})\n",
    "\n",
    "\n",
    "# 距離を抽出する関数（ベクトル化）\n",
    "def extract_distance_column(distance_series):\n",
    "    statuses, distances = [], []\n",
    "    for text in distance_series:\n",
    "        match = distance_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            statuses.append(match.group(1))\n",
    "            distances.append(int(match.group(2)))\n",
    "        else:\n",
    "            statuses.append(None)\n",
    "            distances.append(None)\n",
    "    return pd.DataFrame({'Sta': statuses, 'Dis': distances})\n",
    "\n",
    "\n",
    "# 条件を抽出する関数（ベクトル化）\n",
    "def extract_condition_column(condition_series):\n",
    "    conditions = []\n",
    "    \n",
    "    for text in condition_series:\n",
    "        match = condition_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            conditions.append(match.group(1))\n",
    "        else:\n",
    "            conditions.append(None)\n",
    "    return pd.Series(conditions)\n",
    "\n",
    "\n",
    "# 天気を抽出する関数（ベクトル化）\n",
    "def extract_weather_column(weather_series):\n",
    "    weathers = []\n",
    "    for text in weather_series:\n",
    "        match = weather_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weathers.append(match.group(1))\n",
    "        else:\n",
    "            weathers.append(None)\n",
    "    return pd.Series(weathers)\n",
    "\n",
    "\n",
    "# 体重の変化を抽出する関数\n",
    "def extract_weight_change(weight_series):\n",
    "    weights, changes = [], []\n",
    "    for text in weight_series:\n",
    "        match = re.match(r'(\\d+)\\(([\\+\\-]?\\d+)\\)', text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weights.append(int(match.group(1)))  # 体重\n",
    "            changes.append(int(match.group(2)))  # 変化量\n",
    "        else:\n",
    "            weights.append(None)\n",
    "            changes.append(None)\n",
    "    return pd.DataFrame({'Weight': weights, 'Change': changes})\n",
    "\n",
    "\n",
    "\n",
    "# tqdmの進捗バーを使用して変換\n",
    "tqdm.pandas()\n",
    "df['Time'] = time_to_seconds(df['Time'].values)\n",
    "\n",
    "# SexとAgeの列に分割して格納\n",
    "sex_age_df = extract_sex_age_column(df['Sex/Age'].values)\n",
    "df[['Sex', 'Age']] = sex_age_df\n",
    "\n",
    "# StaとDisの列に分割して格納\n",
    "distance_df = extract_distance_column(df['Distance'].values)\n",
    "df[['Sta', 'Dis']] = distance_df\n",
    "\n",
    "# Conditionの列に分割して格納\n",
    "df['Condition'] = extract_condition_column(df['Condition'].values)\n",
    "\n",
    "# Weatherの列に分割して格納\n",
    "df['Weather'] = extract_weather_column(df['Weather'].values)\n",
    "# 新しいカラム 'Horse Weight' を追加して処理\n",
    "weight_change_df = extract_weight_change(df['Horse Weight'].values)\n",
    "df[['Weight', 'Change']] = weight_change_df\n",
    "\n",
    "# NaN 値が含まれている行をフィルタリング\n",
    "df = df.dropna(subset=['Sex', 'Age', 'Sta', 'Dis', 'Condition', 'Weather'])\n",
    "\n",
    "# 不要な列を削除\n",
    "df = df.drop('Sex/Age', axis=1)\n",
    "df = df.drop('Distance', axis=1)\n",
    "df=df.drop('Horse Weight',axis=1)\n",
    "# フィルタリングしたデータをCSVに保存\n",
    "df.to_csv('../notebook/data-shin/filtered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1486/3910948732.py:5: DtypeWarning: Columns (1,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../notebook/data-shin/filtered_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Frame Rank  Horse Number  Horse Name   Kinryou    Jockey  \\\n",
      "Frame Rank      1.000000      0.883729   -0.011883 -0.000154 -0.005887   \n",
      "Horse Number    0.883729      1.000000    0.052716  0.028397  0.023485   \n",
      "Horse Name     -0.011883      0.052716    1.000000 -0.132046  0.619328   \n",
      "Kinryou        -0.000154      0.028397   -0.132046  1.000000 -0.078536   \n",
      "Jockey         -0.005887      0.023485    0.619328 -0.078536  1.000000   \n",
      "Time            0.000284      0.019110    0.075380  0.309331  0.061148   \n",
      "Nobori          0.019357     -0.086097   -0.067430 -0.310887 -0.035005   \n",
      "Ninki           0.001498      0.078591    0.052998 -0.061918  0.074946   \n",
      "Trainer        -0.003077      0.041090    0.618055 -0.102605  0.525383   \n",
      "Banushi        -0.004301      0.013293    0.593453 -0.110018  0.447008   \n",
      "Date            0.012155     -0.062054   -0.943088  0.134428 -0.673825   \n",
      "Track          -0.023983      0.118151    0.305238  0.038204  0.229157   \n",
      "Weather        -0.002136     -0.017782   -0.011594 -0.009573 -0.017670   \n",
      "Race Number     0.009479      0.047056   -0.007677  0.212467 -0.011571   \n",
      "Sex             0.003178     -0.026659   -0.044771 -0.632052 -0.037548   \n",
      "Age             0.010645     -0.035697    0.018106  0.245616  0.018693   \n",
      "Dis            -0.003897      0.055143    0.116455  0.312394  0.081831   \n",
      "Weight          0.002836      0.043140   -0.033740  0.361406 -0.049894   \n",
      "Change          0.000948     -0.000226   -0.020446  0.009048 -0.014683   \n",
      "\n",
      "                  Time    Nobori     Ninki   Trainer   Banushi      Date  \\\n",
      "Frame Rank    0.000284  0.019357  0.001498 -0.003077 -0.004301  0.012155   \n",
      "Horse Number  0.019110 -0.086097  0.078591  0.041090  0.013293 -0.062054   \n",
      "Horse Name    0.075380 -0.067430  0.052998  0.618055  0.593453 -0.943088   \n",
      "Kinryou       0.309331 -0.310887 -0.061918 -0.102605 -0.110018  0.134428   \n",
      "Jockey        0.061148 -0.035005  0.074946  0.525383  0.447008 -0.673825   \n",
      "Time          1.000000 -0.360421  0.024220  0.059373  0.022206 -0.082432   \n",
      "Nobori       -0.360421  1.000000 -0.045414 -0.051512  0.019182  0.069828   \n",
      "Ninki         0.024220 -0.045414  1.000000  0.071353  0.044213 -0.048997   \n",
      "Trainer       0.059373 -0.051512  0.071353  1.000000  0.460391 -0.665183   \n",
      "Banushi       0.022206  0.019182  0.044213  0.460391  1.000000 -0.622808   \n",
      "Date         -0.082432  0.069828 -0.048997 -0.665183 -0.622808  1.000000   \n",
      "Track         0.133146 -0.286749  0.061966  0.273975  0.165902 -0.340356   \n",
      "Weather      -0.018849  0.022734 -0.002605 -0.027309 -0.002810  0.013593   \n",
      "Race Number   0.160729 -0.092336  0.054874 -0.011328 -0.016995  0.000074   \n",
      "Sex          -0.174348  0.084113  0.031336 -0.041130 -0.043531  0.073500   \n",
      "Age           0.122383 -0.010470  0.067588 -0.005524  0.036567  0.012099   \n",
      "Dis           0.982450 -0.423208  0.039684  0.087379  0.036009 -0.132652   \n",
      "Weight        0.127860 -0.075419 -0.089624 -0.046196 -0.035855  0.042286   \n",
      "Change       -0.003500 -0.010659 -0.018826 -0.014005 -0.015622  0.020329   \n",
      "\n",
      "                 Track   Weather  Race Number       Sex       Age       Dis  \\\n",
      "Frame Rank   -0.023983 -0.002136     0.009479  0.003178  0.010645 -0.003897   \n",
      "Horse Number  0.118151 -0.017782     0.047056 -0.026659 -0.035697  0.055143   \n",
      "Horse Name    0.305238 -0.011594    -0.007677 -0.044771  0.018106  0.116455   \n",
      "Kinryou       0.038204 -0.009573     0.212467 -0.632052  0.245616  0.312394   \n",
      "Jockey        0.229157 -0.017670    -0.011571 -0.037548  0.018693  0.081831   \n",
      "Time          0.133146 -0.018849     0.160729 -0.174348  0.122383  0.982450   \n",
      "Nobori       -0.286749  0.022734    -0.092336  0.084113 -0.010470 -0.423208   \n",
      "Ninki         0.061966 -0.002605     0.054874  0.031336  0.067588  0.039684   \n",
      "Trainer       0.273975 -0.027309    -0.011328 -0.041130 -0.005524  0.087379   \n",
      "Banushi       0.165902 -0.002810    -0.016995 -0.043531  0.036567  0.036009   \n",
      "Date         -0.340356  0.013593     0.000074  0.073500  0.012099 -0.132652   \n",
      "Track         1.000000 -0.050927     0.026123 -0.063640 -0.134274  0.210070   \n",
      "Weather      -0.050927  1.000000     0.007955  0.007184  0.015965 -0.021545   \n",
      "Race Number   0.026123  0.007955     1.000000 -0.126118  0.314575  0.209540   \n",
      "Sex          -0.063640  0.007184    -0.126118  1.000000 -0.109268 -0.190079   \n",
      "Age          -0.134274  0.015965     0.314575 -0.109268  1.000000  0.099554   \n",
      "Dis           0.210070 -0.021545     0.209540 -0.190079  0.099554  1.000000   \n",
      "Weight        0.022382 -0.008228     0.207400 -0.418113  0.201045  0.145754   \n",
      "Change        0.001962  0.005027     0.000576 -0.003544 -0.018160 -0.002672   \n",
      "\n",
      "                Weight    Change  \n",
      "Frame Rank    0.002836  0.000948  \n",
      "Horse Number  0.043140 -0.000226  \n",
      "Horse Name   -0.033740 -0.020446  \n",
      "Kinryou       0.361406  0.009048  \n",
      "Jockey       -0.049894 -0.014683  \n",
      "Time          0.127860 -0.003500  \n",
      "Nobori       -0.075419 -0.010659  \n",
      "Ninki        -0.089624 -0.018826  \n",
      "Trainer      -0.046196 -0.014005  \n",
      "Banushi      -0.035855 -0.015622  \n",
      "Date          0.042286  0.020329  \n",
      "Track         0.022382  0.001962  \n",
      "Weather      -0.008228  0.005027  \n",
      "Race Number   0.207400  0.000576  \n",
      "Sex          -0.418113 -0.003544  \n",
      "Age           0.201045 -0.018160  \n",
      "Dis           0.145754 -0.002672  \n",
      "Weight        1.000000  0.089530  \n",
      "Change        0.089530  1.000000  \n",
      "Top 10 correlated pairs:\n",
      "1. Time - Dis: 0.982\n",
      "2. Horse Name - Date: 0.943\n",
      "3. Frame Rank - Horse Number: 0.884\n",
      "4. Jockey - Date: 0.674\n",
      "5. Trainer - Date: 0.665\n",
      "6. Kinryou - Sex: 0.632\n",
      "7. Banushi - Date: 0.623\n",
      "8. Horse Name - Jockey: 0.619\n",
      "9. Horse Name - Trainer: 0.618\n",
      "10. Horse Name - Banushi: 0.593\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv')\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "# 方法1: 文字列カテゴリを整数に変換\n",
    "df['Horse Name'] = pd.factorize(df['Horse Name'])[0]\n",
    "df['Jockey'] = pd.factorize(df['Jockey'])[0]\n",
    "df['Trainer'] = pd.factorize(df['Trainer'])[0]\n",
    "df['Banushi'] = pd.factorize(df['Banushi'])[0]\n",
    "df['Track'] = pd.factorize(df['Track'])[0]\n",
    "df['Weather'] = pd.factorize(df['Weather'])[0]\n",
    "df['Sex'] = pd.factorize(df['Sex'])[0]\n",
    "\n",
    "# 方法2: 日付データをカテゴリ型に変換する例\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())  # 日付を数値化（連続した数値）\n",
    "\n",
    "#Codeカラムを消す\n",
    "df=df.drop('Code',axis=1)\n",
    "\n",
    "# Weight列の処理 - '5(降)'のような文字列から数値のみを抽出\n",
    "df['Weight'] = df['Weight'].apply(lambda x: float(str(x).split('(')[0]) if pd.notnull(x) else np.nan)\n",
    "df['Change'] = df['Change'].apply(lambda x: float(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 数値型の列のみを選択して相関係数行列を計算\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "print(corr_matrix)\n",
    "# 相関係数の絶対値を取得し、上位10ペアを抽出（対角要素を除く）\n",
    "corr_pairs = []\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        corr_pairs.append({\n",
    "            'pair': (numeric_cols[i], numeric_cols[j]),\n",
    "            'correlation': abs(corr_matrix.iloc[i,j])\n",
    "        })\n",
    "\n",
    "# 相関係数の絶対値で降順ソート\n",
    "corr_pairs = sorted(corr_pairs, key=lambda x: x['correlation'], reverse=True)\n",
    "\n",
    "# 上位10ペアを表示\n",
    "print(\"Top 10 correlated pairs:\")\n",
    "for i, pair in enumerate(corr_pairs[:10], 1):\n",
    "    print(f\"{i}. {pair['pair'][0]} - {pair['pair'][1]}: {pair['correlation']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習でタイム予測(100000行)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9668779847011264\n",
      "Mean Absolute Error: 0.7414509715635068\n",
      "        Actual Time  Predicted Time\n",
      "440484         93.2          92.282\n",
      "170130         75.4          76.233\n",
      "84159          75.8          75.283\n",
      "272923         84.3          85.756\n",
      "195556         88.7          88.503\n",
      "265601         98.1          97.176\n",
      "63784          99.1          99.743\n",
      "485157        103.8         105.138\n",
      "385200         69.0          69.320\n",
      "14127          78.2          77.455\n",
      "        Actual Time  Predicted Time\n",
      "58116          93.8          93.474\n",
      "186338         93.5          94.619\n",
      "257155         91.3          90.962\n",
      "336424         92.4          93.232\n",
      "30412         114.3         112.933\n",
      "116819        116.2         115.932\n",
      "459483         93.5          92.864\n",
      "432530         85.7          87.283\n",
      "299024         78.5          79.233\n",
      "158697         77.5          77.492\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=500000)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code'])\n",
    "\n",
    "# 确保所有的特征列都是数值型\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 予測結果と実際のタイムの差を計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    # 予測タイムと実際のタイムを表示\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    \n",
    "    \n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU加速on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2145.28s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cuml-cu117 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cuml-cu117\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2151.58s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting cupy-cuda117\n",
      "  Downloading cupy_cuda117-10.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting numpy<1.25,>=1.18 (from cupy-cuda117)\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda117)\n",
      "  Downloading fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
      "Downloading cupy_cuda117-10.6.0-cp310-cp310-manylinux1_x86_64.whl (81.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.7/81.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (51 kB)\n",
      "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastrlock, numpy, cupy-cuda117\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed cupy-cuda117-10.6.0 fastrlock-0.8.2 numpy-1.24.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2230.48s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ximihu/.local/lib/python3.10/site-packages (4.67.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cuml-cu117\n",
    "%pip install cupy-cuda117\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CuPy and cuML are available\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cuml.ensemble import RandomForestRegressor as cuRF\n",
    "    use_gpu = True\n",
    "except ImportError as e:\n",
    "    print(\"CuPy or cuML is not available. Falling back to CPU.\")\n",
    "    use_gpu = False\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Convert necessary categorical variables to numeric\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# Convert date data to ordinal\n",
    "try:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "except Exception as e:\n",
    "    print(f\"Error processing date data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Process Weight column\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Process Change column\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Set target and features\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code'])\n",
    "\n",
    "# Ensure all feature columns are numeric\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training and prediction\n",
    "try:\n",
    "    if use_gpu:\n",
    "        # Use GPU for RandomForestRegressor\n",
    "        model = cuRF(n_estimators=100, random_state=42)\n",
    "        for _ in tqdm(range(1), desc=\"Training Model\"):\n",
    "            model.fit(cp.asarray(X_train), cp.asarray(y_train))\n",
    "        check_is_fitted(model)\n",
    "        y_pred = model.predict(cp.asarray(X_test)).get()\n",
    "    else:\n",
    "        # Use CPU for RandomForestRegressor\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        for _ in tqdm(range(1), desc=\"Training Model\"):\n",
    "            model.fit(X_train, y_train)\n",
    "        check_is_fitted(model)\n",
    "        y_pred = model.predict(X_test)\n",
    "except (ValueError, AttributeError, Exception) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")\n",
    "    raise\n",
    "\n",
    "# Calculate and print Mean Squared Error\n",
    "try:\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating Mean Squared Error: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
