{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sfsdfsdおれ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ximihu/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ximihu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ximihu/.local/lib/python3.10/site-packages (4.67.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ximihu/.local/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install tqdm\n",
    "%pip install re\n",
    "%pip install sklearn\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125/1400195009.py:9: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_b = pd.read_csv(data_b_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# データAとデータBのCSVファイルのパス\n",
    "data_a_path = '../data/original_data/race_table/combined.csv'  # データAのCSVファイルパス\n",
    "data_b_path = '../data/merged_data/merged_sorted.csv'  # データBのCSVファイルパス\n",
    "\n",
    "# CSVファイルからデータを読み込む\n",
    "data_a = pd.read_csv(data_a_path,low_memory=False)\n",
    "data_b = pd.read_csv(data_b_path)\n",
    "data_a=data_a.dropna()\n",
    "data_b=data_b.dropna()\n",
    "data_a=data_a.drop(columns=['first_horses', 'second_horses', 'third_horses', 'Time', 'Jockey', 'Trainer'])\n",
    "# Code列でデータを結合 (data_bにdata_aの情報を追加)\n",
    "merged_data = pd.merge(data_b, data_a, on='Code', how='left')\n",
    "\n",
    "#結果の確認\n",
    "merged_data.to_csv('../notebook/data-shin/merged_data_final.csv', index=False)\n",
    "for i in merged_data.columns:\n",
    "    # 各列のユニークな値とその出現回数を取得\n",
    "    value_counts = df[i].value_counts()  # 各値とそのカウント\n",
    "    print(f\"{i}: {value_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量の析出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125/1133966646.py:7: DtypeWarning: Columns (1,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = '../notebook/data-shin/merged_data_final.csv'\n",
    "# CSVファイルの読み込み\n",
    "df = pd.read_csv(file_path)\n",
    "# 12桁のコードのみを抽出\n",
    "df = df[df['Code'].astype(str).str.match(r'^\\d{12}$')]\n",
    "\n",
    "# 正規表現の事前コンパイル（前後にある任意の空白を無視）\n",
    "time_pattern1 = re.compile(r'\\s*(\\d+):(\\d+\\.\\d+)\\s*')\n",
    "time_pattern2 = re.compile(r'\\s*(\\d+):(\\d+):(\\d+)\\s*')\n",
    "sex_age_pattern = re.compile(r'\\s*([牡牝セ])(\\d+)\\s*')\n",
    "distance_pattern = re.compile(r'\\s*([ダ芝障])(\\d+)\\s*')\n",
    "condition_pattern = re.compile(r'\\s*([良重稍不])\\s*')\n",
    "weather_pattern = re.compile(r'\\s*(晴|曇|小雨|雨|雪|小雪)\\s*')\n",
    "kinryou_pattern = re.compile(r'(\\d+)\\(([\\+\\-]?\\d+)\\)')\n",
    "\n",
    "\n",
    "# 時間を秒に変換する関数 (ベクトル化)\n",
    "def time_to_seconds(time_series):\n",
    "    result = []\n",
    "    for time_str in time_series:\n",
    "        if isinstance(time_str, str):\n",
    "            match1 = time_pattern1.match(time_str)\n",
    "            if match1:\n",
    "                minutes, seconds = int(match1.group(1)), float(match1.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "            match2 = time_pattern2.match(time_str)\n",
    "            if match2:\n",
    "                minutes, seconds = int(match2.group(1)), float(match2.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "        result.append(None)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 性別と年齢を抽出する関数（ベクトル化）\n",
    "def extract_sex_age_column(sex_age_series):\n",
    "    sexes, ages = [], []\n",
    "    for text in sex_age_series:\n",
    "        match = sex_age_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            sex = match.group(1)\n",
    "            if sex in ['牡', '牝', 'セ']:  # 性別が牡、牝、セのみを許可\n",
    "                sexes.append(sex)\n",
    "                ages.append(int(match.group(2)))\n",
    "            else:\n",
    "                sexes.append(None)\n",
    "                ages.append(None)\n",
    "        else:\n",
    "            sexes.append(None)\n",
    "            ages.append(None)\n",
    "    return pd.DataFrame({'Sex': sexes, 'Age': ages})\n",
    "\n",
    "\n",
    "# 距離を抽出する関数（ベクトル化）\n",
    "def extract_distance_column(distance_series):\n",
    "    statuses, distances = [], []\n",
    "    for text in distance_series:\n",
    "        match = distance_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            statuses.append(match.group(1))\n",
    "            distances.append(int(match.group(2)))\n",
    "        else:\n",
    "            statuses.append(None)\n",
    "            distances.append(None)\n",
    "    return pd.DataFrame({'Sta': statuses, 'Dis': distances})\n",
    "\n",
    "\n",
    "# 条件を抽出する関数（ベクトル化）\n",
    "def extract_condition_column(condition_series):\n",
    "    conditions = []\n",
    "    \n",
    "    for text in condition_series:\n",
    "        match = condition_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            conditions.append(match.group(1))\n",
    "        else:\n",
    "            conditions.append(None)\n",
    "    return pd.Series(conditions)\n",
    "\n",
    "\n",
    "# 天気を抽出する関数（ベクトル化）\n",
    "def extract_weather_column(weather_series):\n",
    "    weathers = []\n",
    "    for text in weather_series:\n",
    "        match = weather_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weathers.append(match.group(1))\n",
    "        else:\n",
    "            weathers.append(None)\n",
    "    return pd.Series(weathers)\n",
    "\n",
    "\n",
    "# 体重の変化を抽出する関数\n",
    "def extract_weight_change(weight_series):\n",
    "    weights, changes = [], []\n",
    "    for text in weight_series:\n",
    "        match = re.match(r'(\\d+)\\(([\\+\\-]?\\d+)\\)', text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weights.append(int(match.group(1)))  # 体重\n",
    "            changes.append(int(match.group(2)))  # 変化量\n",
    "        else:\n",
    "            weights.append(None)\n",
    "            changes.append(None)\n",
    "    return pd.DataFrame({'Weight': weights, 'Change': changes})\n",
    "\n",
    "df['Time'] = time_to_seconds(df['Time'].values)\n",
    "\n",
    "# SexとAgeの列に分割して格納\n",
    "sex_age_df = extract_sex_age_column(df['Sex/Age'].values)\n",
    "df[['Sex', 'Age']] = sex_age_df\n",
    "\n",
    "# StaとDisの列に分割して格納\n",
    "distance_df = extract_distance_column(df['Distance'].values)\n",
    "df[['Sta', 'Dis']] = distance_df\n",
    "\n",
    "# Conditionの列に分割して格納\n",
    "df['Condition'] = extract_condition_column(df['Condition'].values)\n",
    "\n",
    "# Weatherの列に分割して格納\n",
    "df['Weather'] = extract_weather_column(df['Weather'].values)\n",
    "# 新しいカラム 'Horse Weight' を追加して処理\n",
    "weight_change_df = extract_weight_change(df['Horse Weight'].values)\n",
    "df[['Weight', 'Change']] = weight_change_df\n",
    "\n",
    "# NaN 値が含まれている行をフィルタリング\n",
    "df = df.dropna(subset=['Sex', 'Age', 'Sta', 'Dis', 'Condition', 'Weather'])\n",
    "\n",
    "# 不要な列を削除\n",
    "df = df.drop('Sex/Age', axis=1)\n",
    "df = df.drop('Distance', axis=1)\n",
    "df=df.drop('Horse Weight',axis=1)\n",
    "# フィルタリングしたデータをCSVに保存\n",
    "df.to_csv('../notebook/data-shin/filtered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40468/1983141988.py:5: DtypeWarning: Columns (1,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../notebook/data-shin/filtered_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Frame Rank  Horse Name   Kinryou    Jockey      Time    Nobori  \\\n",
      "Frame Rank     1.000000   -0.011883 -0.000154 -0.005887  0.000284  0.019357   \n",
      "Horse Name    -0.011883    1.000000 -0.132046  0.619328  0.075380 -0.067430   \n",
      "Kinryou       -0.000154   -0.132046  1.000000 -0.078536  0.309331 -0.310887   \n",
      "Jockey        -0.005887    0.619328 -0.078536  1.000000  0.061148 -0.035005   \n",
      "Time           0.000284    0.075380  0.309331  0.061148  1.000000 -0.360421   \n",
      "Nobori         0.019357   -0.067430 -0.310887 -0.035005 -0.360421  1.000000   \n",
      "Ninki          0.001498    0.052998 -0.061918  0.074946  0.024220 -0.045414   \n",
      "Trainer       -0.003077    0.618055 -0.102605  0.525383  0.059373 -0.051512   \n",
      "Banushi       -0.004301    0.593453 -0.110018  0.447008  0.022206  0.019182   \n",
      "Date           0.012155   -0.943088  0.134428 -0.673825 -0.082432  0.069828   \n",
      "Track         -0.023983    0.305238  0.038204  0.229157  0.133146 -0.286749   \n",
      "Weather       -0.002136   -0.011594 -0.009573 -0.017670 -0.018849  0.022734   \n",
      "Race Number    0.009479   -0.007677  0.212467 -0.011571  0.160729 -0.092336   \n",
      "Sex            0.003178   -0.044771 -0.632052 -0.037548 -0.174348  0.084113   \n",
      "Age            0.010645    0.018106  0.245616  0.018693  0.122383 -0.010470   \n",
      "Dis           -0.003897    0.116455  0.312394  0.081831  0.982450 -0.423208   \n",
      "Weight         0.002836   -0.033740  0.361406 -0.049894  0.127860 -0.075419   \n",
      "Change         0.000948   -0.020446  0.009048 -0.014683 -0.003500 -0.010659   \n",
      "\n",
      "                Ninki   Trainer   Banushi      Date     Track   Weather  \\\n",
      "Frame Rank   0.001498 -0.003077 -0.004301  0.012155 -0.023983 -0.002136   \n",
      "Horse Name   0.052998  0.618055  0.593453 -0.943088  0.305238 -0.011594   \n",
      "Kinryou     -0.061918 -0.102605 -0.110018  0.134428  0.038204 -0.009573   \n",
      "Jockey       0.074946  0.525383  0.447008 -0.673825  0.229157 -0.017670   \n",
      "Time         0.024220  0.059373  0.022206 -0.082432  0.133146 -0.018849   \n",
      "Nobori      -0.045414 -0.051512  0.019182  0.069828 -0.286749  0.022734   \n",
      "Ninki        1.000000  0.071353  0.044213 -0.048997  0.061966 -0.002605   \n",
      "Trainer      0.071353  1.000000  0.460391 -0.665183  0.273975 -0.027309   \n",
      "Banushi      0.044213  0.460391  1.000000 -0.622808  0.165902 -0.002810   \n",
      "Date        -0.048997 -0.665183 -0.622808  1.000000 -0.340356  0.013593   \n",
      "Track        0.061966  0.273975  0.165902 -0.340356  1.000000 -0.050927   \n",
      "Weather     -0.002605 -0.027309 -0.002810  0.013593 -0.050927  1.000000   \n",
      "Race Number  0.054874 -0.011328 -0.016995  0.000074  0.026123  0.007955   \n",
      "Sex          0.031336 -0.041130 -0.043531  0.073500 -0.063640  0.007184   \n",
      "Age          0.067588 -0.005524  0.036567  0.012099 -0.134274  0.015965   \n",
      "Dis          0.039684  0.087379  0.036009 -0.132652  0.210070 -0.021545   \n",
      "Weight      -0.089624 -0.046196 -0.035855  0.042286  0.022382 -0.008228   \n",
      "Change      -0.018826 -0.014005 -0.015622  0.020329  0.001962  0.005027   \n",
      "\n",
      "             Race Number       Sex       Age       Dis    Weight    Change  \n",
      "Frame Rank      0.009479  0.003178  0.010645 -0.003897  0.002836  0.000948  \n",
      "Horse Name     -0.007677 -0.044771  0.018106  0.116455 -0.033740 -0.020446  \n",
      "Kinryou         0.212467 -0.632052  0.245616  0.312394  0.361406  0.009048  \n",
      "Jockey         -0.011571 -0.037548  0.018693  0.081831 -0.049894 -0.014683  \n",
      "Time            0.160729 -0.174348  0.122383  0.982450  0.127860 -0.003500  \n",
      "Nobori         -0.092336  0.084113 -0.010470 -0.423208 -0.075419 -0.010659  \n",
      "Ninki           0.054874  0.031336  0.067588  0.039684 -0.089624 -0.018826  \n",
      "Trainer        -0.011328 -0.041130 -0.005524  0.087379 -0.046196 -0.014005  \n",
      "Banushi        -0.016995 -0.043531  0.036567  0.036009 -0.035855 -0.015622  \n",
      "Date            0.000074  0.073500  0.012099 -0.132652  0.042286  0.020329  \n",
      "Track           0.026123 -0.063640 -0.134274  0.210070  0.022382  0.001962  \n",
      "Weather         0.007955  0.007184  0.015965 -0.021545 -0.008228  0.005027  \n",
      "Race Number     1.000000 -0.126118  0.314575  0.209540  0.207400  0.000576  \n",
      "Sex            -0.126118  1.000000 -0.109268 -0.190079 -0.418113 -0.003544  \n",
      "Age             0.314575 -0.109268  1.000000  0.099554  0.201045 -0.018160  \n",
      "Dis             0.209540 -0.190079  0.099554  1.000000  0.145754 -0.002672  \n",
      "Weight          0.207400 -0.418113  0.201045  0.145754  1.000000  0.089530  \n",
      "Change          0.000576 -0.003544 -0.018160 -0.002672  0.089530  1.000000  \n",
      "Top 10 correlated pairs:\n",
      "1. Time - Dis: 0.982\n",
      "2. Horse Name - Date: 0.943\n",
      "3. Jockey - Date: 0.674\n",
      "4. Trainer - Date: 0.665\n",
      "5. Kinryou - Sex: 0.632\n",
      "6. Banushi - Date: 0.623\n",
      "7. Horse Name - Jockey: 0.619\n",
      "8. Horse Name - Trainer: 0.618\n",
      "9. Horse Name - Banushi: 0.593\n",
      "10. Jockey - Trainer: 0.525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv')\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "# 方法1: 文字列カテゴリを整数に変換\n",
    "df['Horse Name'] = pd.factorize(df['Horse Name'])[0]\n",
    "df['Jockey'] = pd.factorize(df['Jockey'])[0]\n",
    "df['Trainer'] = pd.factorize(df['Trainer'])[0]\n",
    "df['Banushi'] = pd.factorize(df['Banushi'])[0]\n",
    "df['Track'] = pd.factorize(df['Track'])[0]\n",
    "df['Weather'] = pd.factorize(df['Weather'])[0]\n",
    "df['Sex'] = pd.factorize(df['Sex'])[0]\n",
    "\n",
    "# 方法2: 日付データをカテゴリ型に変換する例\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())  # 日付を数値化（連続した数値）\n",
    "\n",
    "#Codeカラムを消す\n",
    "df=df.drop('Code',axis=1)\n",
    "df=df.drop('Horse Number',axis=1)\n",
    "\n",
    "# Weight列の処理 - '5(降)'のような文字列から数値のみを抽出\n",
    "df['Weight'] = df['Weight'].apply(lambda x: float(str(x).split('(')[0]) if pd.notnull(x) else np.nan)\n",
    "df['Change'] = df['Change'].apply(lambda x: float(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 数値型の列のみを選択して相関係数行列を計算\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "print(corr_matrix)\n",
    "# 相関係数の絶対値を取得し、上位10ペアを抽出（対角要素を除く）\n",
    "corr_pairs = []\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        corr_pairs.append({\n",
    "            'pair': (numeric_cols[i], numeric_cols[j]),\n",
    "            'correlation': abs(corr_matrix.iloc[i,j])\n",
    "        })\n",
    "\n",
    "# 相関係数の絶対値で降順ソート\n",
    "corr_pairs = sorted(corr_pairs, key=lambda x: x['correlation'], reverse=True)\n",
    "\n",
    "# 上位10ペアを表示\n",
    "print(\"Top 10 correlated pairs:\")\n",
    "for i, pair in enumerate(corr_pairs[:10], 1):\n",
    "    print(f\"{i}. {pair['pair'][0]} - {pair['pair'][1]}: {pair['correlation']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.5553754562406237\n",
      "Mean Absolute Error: 0.9086452096877902\n",
      "R2 Score: 0.9962767496176523\n",
      "       Actual Time  Predicted Time\n",
      "14558         75.5          74.344\n",
      "47286         85.0          84.635\n",
      "28091         89.7          90.865\n",
      "29606         76.0          75.735\n",
      "54587         91.9          91.940\n",
      "44941        112.6         109.795\n",
      "58314         82.2          82.617\n",
      "34716        112.6         111.997\n",
      "39026        104.2         104.913\n",
      "13444        120.8         120.623\n",
      "       Actual Time  Predicted Time\n",
      "5755         118.4         118.585\n",
      "22825         93.0          92.837\n",
      "11186         88.5          88.651\n",
      "31937        107.6         106.935\n",
      "68286         73.8          73.315\n",
      "52256         81.0          81.832\n",
      "14015         96.0          95.076\n",
      "60488         94.9          96.594\n",
      "3622          92.1          92.799\n",
      "19656         69.9          69.921\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# データの読み込み\n",
    "try:\n",
    "    df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=70000)\n",
    "except Exception as e:\n",
    "    print(f\"Error while reading the data: {e}\")\n",
    "\n",
    "# 必要なカテゴリ変数にOne-Hotエンコーディングを適用\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "try:\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error while applying One-Hot Encoding: {e}\")\n",
    "\n",
    "# 日付データを数値化\n",
    "try:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "except Exception as e:\n",
    "    print(f\"Error while converting date to ordinal: {e}\")\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "try:\n",
    "    df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "except Exception as e:\n",
    "    print(f\"Error while processing Weight column: {e}\")\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "try:\n",
    "    df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "except Exception as e:\n",
    "    print(f\"Error while processing Change column: {e}\")\n",
    "\n",
    "# 欠損値を含む行を削除（補完方法も検討）\n",
    "try:\n",
    "    df = df.dropna()  # もしデータが不足している場合、欠損値の補完も考慮する\n",
    "except Exception as e:\n",
    "    print(f\"Error while dropping rows with missing values: {e}\")\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code', 'Horse Number'])\n",
    "\n",
    "# 確保するために、説明変数はすべて数値型に\n",
    "try:\n",
    "    features = features.select_dtypes(include=[np.number])\n",
    "except Exception as e:\n",
    "    print(f\"Error while selecting numeric features: {e}\")\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "except Exception as e:\n",
    "    print(f\"Error while splitting the data into training and testing sets: {e}\")\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 予測結果と実際のタイムの差を計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    # 予測タイムと実際のタイムを表示\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    \n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習でタイム予測(500000行)(ランダムフォレスト)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9647742792619719\n",
      "Mean Absolute Error: 0.7408475315683096\n",
      "R2 Score: 0.9975617383110674\n",
      "        Actual Time  Predicted Time\n",
      "440484         93.2          92.290\n",
      "170130         75.4          76.226\n",
      "84159          75.8          75.233\n",
      "272923         84.3          85.704\n",
      "195556         88.7          88.479\n",
      "265601         98.1          97.176\n",
      "63784          99.1          99.643\n",
      "485157        103.8         105.160\n",
      "385200         69.0          69.347\n",
      "14127          78.2          77.399\n",
      "        Actual Time  Predicted Time\n",
      "58116          93.8          93.299\n",
      "186338         93.5          94.570\n",
      "257155         91.3          91.065\n",
      "336424         92.4          93.133\n",
      "30412         114.3         113.148\n",
      "116819        116.2         116.077\n",
      "459483         93.5          92.921\n",
      "432530         85.7          87.369\n",
      "299024         78.5          79.309\n",
      "158697         77.5          77.423\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=500000)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code','Horse Number'])\n",
    "\n",
    "# 确保所有的特征列都是数值型\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 予測結果と実際のタイムの差を計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    # 予測タイムと実際のタイムを表示\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    \n",
    "    \n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU加速on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[39 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/requirements.py\", line 36, in __init__\n",
      "  \u001b[31m   \u001b[0m     parsed = _parse_requirement(requirement_string)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 62, in parse_requirement\n",
      "  \u001b[31m   \u001b[0m     return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 80, in _parse_requirement\n",
      "  \u001b[31m   \u001b[0m     url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 124, in _parse_requirement_details\n",
      "  \u001b[31m   \u001b[0m     marker = _parse_requirement_marker(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_parser.py\", line 145, in _parse_requirement_marker\n",
      "  \u001b[31m   \u001b[0m     tokenizer.raise_syntax_error(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/_tokenizer.py\", line 167, in raise_syntax_error\n",
      "  \u001b[31m   \u001b[0m     raise ParserSyntaxError(\n",
      "  \u001b[31m   \u001b[0m packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-9xe96ht1/tensorflow-gpu_6e220518616842a49a60ab31bd6ec4cb/setup.py\", line 40, in <module>\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/__init__.py\", line 116, in setup\n",
      "  \u001b[31m   \u001b[0m     _install_setup_requires(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/__init__.py\", line 87, in _install_setup_requires\n",
      "  \u001b[31m   \u001b[0m     dist.parse_config_files(ignore_option_errors=True)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/dist.py\", line 657, in parse_config_files\n",
      "  \u001b[31m   \u001b[0m     self._finalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/dist.py\", line 387, in _finalize_requires\n",
      "  \u001b[31m   \u001b[0m     self._normalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/setuptools/dist.py\", line 402, in _normalize_requires\n",
      "  \u001b[31m   \u001b[0m     self.install_requires = list(map(str, _reqs.parse(install_requires)))\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ximihu/.local/lib/python3.10/site-packages/packaging/requirements.py\", line 38, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise InvalidRequirement(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ximihu/.local/lib/python3.10/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ximihu/.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ximihu/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-gpu\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('../notebook/data-shin/filtered_data.csv', low_memory=False, nrows=500000)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code','Horse Number'])\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# ランダムフォレスト回帰モデルの構築\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "try:\n",
    "    # モデルの訓練\n",
    "    model.fit(X_train, y_train)\n",
    "    check_is_fitted(model)\n",
    "\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 評価指標の計算\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "\n",
    "    # クロスバリデーションの実施（MAEを評価）\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    print(f\"Cross-Validation MAE: {-np.mean(cv_scores)}\")\n",
    "\n",
    "    # 実際のタイムと予測タイムの比較\n",
    "    comparison_df = pd.DataFrame({'Actual Time': y_test, 'Predicted Time': y_pred})\n",
    "    print(comparison_df.head(10))\n",
    "    print(comparison_df.tail(10))\n",
    "\n",
    "    # 実際のタイムと予測タイムの散布図\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel(\"Actual Time\")\n",
    "    plt.ylabel(\"Predicted Time\")\n",
    "    plt.title(\"Actual vs Predicted Time\")\n",
    "    plt.show()\n",
    "\n",
    "    # 残差の分布をヒストグラムで可視化\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=50, alpha=0.7)\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "except (ValueError, AttributeError) as e:\n",
    "    print(f\"Error during model fitting or prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/ximihu/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /home/ximihu/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/ximihu/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/ximihu/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ximihu/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ximihu/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ximihu/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ximihu/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ximihu/.local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ximihu/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ximihu/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ximihu/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ximihu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 18:49:20.952360: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 18:49:21.061877: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 18:49:21.159098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731923361.290143    4946 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731923361.326640    4946 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 18:49:21.560855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflowで機械学習 モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2636/818953571.py:10: DtypeWarning: Columns (1,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ximihu/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 961us/step - loss: 63241136.0000 - val_loss: 80190.7578\n",
      "Epoch 2/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 963us/step - loss: 30030.9277 - val_loss: 8704.3730\n",
      "Epoch 3/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 964us/step - loss: 1263.7177 - val_loss: 120.5587\n",
      "Epoch 4/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 970us/step - loss: 90.7938 - val_loss: 28.0929\n",
      "Epoch 5/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 973us/step - loss: 24.2224 - val_loss: 33.2676\n",
      "Epoch 6/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 972us/step - loss: 17.4873 - val_loss: 13.0512\n",
      "Epoch 7/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 981us/step - loss: 16.1255 - val_loss: 11.9093\n",
      "Epoch 8/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1ms/step - loss: 15.3413 - val_loss: 12.5375\n",
      "Epoch 9/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1ms/step - loss: 14.2409 - val_loss: 29.4864\n",
      "Epoch 10/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1ms/step - loss: 14.6003 - val_loss: 18.7091\n",
      "Epoch 11/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1ms/step - loss: 13.6336 - val_loss: 14.9969\n",
      "Epoch 12/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1ms/step - loss: 12.9877 - val_loss: 10.1883\n",
      "Epoch 13/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1ms/step - loss: 11.6976 - val_loss: 9.4520\n",
      "Epoch 14/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1ms/step - loss: 10.8207 - val_loss: 11.0837\n",
      "Epoch 15/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1ms/step - loss: 12.5414 - val_loss: 10.2527\n",
      "Epoch 16/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1ms/step - loss: 12.2795 - val_loss: 13.1838\n",
      "Epoch 17/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1ms/step - loss: 12.0190 - val_loss: 13.0535\n",
      "Epoch 18/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 990us/step - loss: 11.0901 - val_loss: 9.1008\n",
      "Epoch 19/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 975us/step - loss: 10.5174 - val_loss: 9.5955\n",
      "Epoch 20/20\n",
      "\u001b[1m33572/33572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 975us/step - loss: 10.4636 - val_loss: 8.7856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8393/8393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 692us/step\n",
      "Mean Squared Error: 8.78554590614421\n",
      "Mean Absolute Error: 2.2518303626471488\n",
      "R2 Score: 0.9804887046941517\n",
      "   Actual Time  Predicted Time\n",
      "0         99.2       97.713959\n",
      "1        122.8      123.613907\n",
      "2        107.7      105.159180\n",
      "3         94.0       91.678055\n",
      "4         90.2       89.062775\n",
      "5         93.2       91.141563\n",
      "6         74.6       74.598030\n",
      "7        145.6      153.934418\n",
      "8         89.7       88.289574\n",
      "9         87.0       84.652481\n",
      "        Actual Time  Predicted Time\n",
      "268561         88.9       85.875458\n",
      "268562         61.9       60.985794\n",
      "268563         79.1       78.088165\n",
      "268564         54.9       60.630394\n",
      "268565         99.0       97.765419\n",
      "268566         97.3       96.167770\n",
      "268567         71.4       73.363899\n",
      "268568        106.5      107.540054\n",
      "268569         89.2       85.317955\n",
      "268570         91.9       90.096123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import re\n",
    "import tqdm\n",
    "file_path = '../notebook/data-shin/merged_data_final.csv'\n",
    "# CSVファイルの読み込み\n",
    "df = pd.read_csv(file_path)\n",
    "# 12桁のコードのみを抽出\n",
    "df = df[df['Code'].astype(str).str.match(r'^\\d{12}$')]\n",
    "\n",
    "# 正規表現の事前コンパイル（前後にある任意の空白を無視）\n",
    "time_pattern1 = re.compile(r'\\s*(\\d+):(\\d+\\.\\d+)\\s*')\n",
    "time_pattern2 = re.compile(r'\\s*(\\d+):(\\d+):(\\d+)\\s*')\n",
    "sex_age_pattern = re.compile(r'\\s*([牡牝セ])(\\d+)\\s*')\n",
    "distance_pattern = re.compile(r'\\s*([ダ芝障])(\\d+)\\s*')\n",
    "condition_pattern = re.compile(r'\\s*([良重稍不])\\s*')\n",
    "weather_pattern = re.compile(r'\\s*(晴|曇|小雨|雨|雪|小雪)\\s*')\n",
    "kinryou_pattern = re.compile(r'(\\d+)\\(([\\+\\-]?\\d+)\\)')\n",
    "\n",
    "\n",
    "# 時間を秒に変換する関数 (ベクトル化)\n",
    "def time_to_seconds(time_series):\n",
    "    result = []\n",
    "    for time_str in time_series:\n",
    "        if isinstance(time_str, str):\n",
    "            match1 = time_pattern1.match(time_str)\n",
    "            if match1:\n",
    "                minutes, seconds = int(match1.group(1)), float(match1.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "            match2 = time_pattern2.match(time_str)\n",
    "            if match2:\n",
    "                minutes, seconds = int(match2.group(1)), float(match2.group(2))\n",
    "                result.append(minutes * 60 + seconds)\n",
    "                continue\n",
    "        result.append(None)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 性別と年齢を抽出する関数（ベクトル化）\n",
    "def extract_sex_age_column(sex_age_series):\n",
    "    sexes, ages = [], []\n",
    "    for text in sex_age_series:\n",
    "        match = sex_age_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            sex = match.group(1)\n",
    "            if sex in ['牡', '牝', 'セ']:  # 性別が牡、牝、セのみを許可\n",
    "                sexes.append(sex)\n",
    "                ages.append(int(match.group(2)))\n",
    "            else:\n",
    "                sexes.append(None)\n",
    "                ages.append(None)\n",
    "        else:\n",
    "            sexes.append(None)\n",
    "            ages.append(None)\n",
    "    return pd.DataFrame({'Sex': sexes, 'Age': ages})\n",
    "\n",
    "\n",
    "# 距離を抽出する関数（ベクトル化）\n",
    "def extract_distance_column(distance_series):\n",
    "    statuses, distances = [], []\n",
    "    for text in distance_series:\n",
    "        match = distance_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            statuses.append(match.group(1))\n",
    "            distances.append(int(match.group(2)))\n",
    "        else:\n",
    "            statuses.append(None)\n",
    "            distances.append(None)\n",
    "    return pd.DataFrame({'Sta': statuses, 'Dis': distances})\n",
    "\n",
    "\n",
    "# 条件を抽出する関数（ベクトル化）\n",
    "def extract_condition_column(condition_series):\n",
    "    conditions = []\n",
    "    \n",
    "    for text in condition_series:\n",
    "        match = condition_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            conditions.append(match.group(1))\n",
    "        else:\n",
    "            conditions.append(None)\n",
    "    return pd.Series(conditions)\n",
    "\n",
    "\n",
    "# 天気を抽出する関数（ベクトル化）\n",
    "def extract_weather_column(weather_series):\n",
    "    weathers = []\n",
    "    for text in weather_series:\n",
    "        match = weather_pattern.match(text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weathers.append(match.group(1))\n",
    "        else:\n",
    "            weathers.append(None)\n",
    "    return pd.Series(weathers)\n",
    "\n",
    "\n",
    "# 体重の変化を抽出する関数\n",
    "def extract_weight_change(weight_series):\n",
    "    weights, changes = [], []\n",
    "    for text in weight_series:\n",
    "        match = re.match(r'(\\d+)\\(([\\+\\-]?\\d+)\\)', text) if isinstance(text, str) else None\n",
    "        if match:\n",
    "            weights.append(int(match.group(1)))  # 体重\n",
    "            changes.append(int(match.group(2)))  # 変化量\n",
    "        else:\n",
    "            weights.append(None)\n",
    "            changes.append(None)\n",
    "    return pd.DataFrame({'Weight': weights, 'Change': changes})\n",
    "\n",
    "\n",
    "\n",
    "df['Time'] = time_to_seconds(df['Time'].values)\n",
    "\n",
    "# SexとAgeの列に分割して格納\n",
    "sex_age_df = extract_sex_age_column(df['Sex/Age'].values)\n",
    "df[['Sex', 'Age']] = sex_age_df\n",
    "\n",
    "# StaとDisの列に分割して格納\n",
    "distance_df = extract_distance_column(df['Distance'].values)\n",
    "df[['Sta', 'Dis']] = distance_df\n",
    "\n",
    "# Conditionの列に分割して格納\n",
    "df['Condition'] = extract_condition_column(df['Condition'].values)\n",
    "\n",
    "# Weatherの列に分割して格納\n",
    "df['Weather'] = extract_weather_column(df['Weather'].values)\n",
    "# 新しいカラム 'Horse Weight' を追加して処理\n",
    "weight_change_df = extract_weight_change(df['Horse Weight'].values)\n",
    "df[['Weight', 'Change']] = weight_change_df\n",
    "\n",
    "# NaN 値が含まれている行をフィルタリング\n",
    "df = df.dropna(subset=['Sex', 'Age', 'Sta', 'Dis', 'Condition', 'Weather'])\n",
    "\n",
    "# 不要な列を削除\n",
    "df = df.drop('Sex/Age', axis=1)\n",
    "df = df.drop('Distance', axis=1)\n",
    "df=df.drop('Horse Weight',axis=1)\n",
    "\n",
    "# 必要なカテゴリ変数の数値化\n",
    "categorical_columns = ['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Track', 'Weather', 'Sex']\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.factorize(df[column])[0]\n",
    "\n",
    "# 日付データを数値化\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].map(lambda date: date.toordinal())\n",
    "\n",
    "# Weight列の処理\n",
    "def extract_weight(value):\n",
    "    try:\n",
    "        return float(str(value).split('(')[0])\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Weight'] = df['Weight'].apply(lambda x: extract_weight(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Change列の処理\n",
    "def extract_change(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df['Change'] = df['Change'].apply(lambda x: extract_change(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "df = df.dropna()\n",
    "\n",
    "# 目的変数と説明変数の設定\n",
    "target = 'Time'\n",
    "features = df.drop(columns=[target, 'Code','Horse Number'])\n",
    "\n",
    "# 确保所有的特征列都是数値型\n",
    "features = features.select_dtypes(include=[np.number])\n",
    "\n",
    "# データを訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# モデルの定義\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# モデルの訓練\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# モデルの保存\n",
    "model.save('trained_model.h5')\n",
    "\n",
    "# テストデータに対する予測\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 予測結果と実際のタイムの差を計算\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "# 予測タイムと実際のタイムを表示\n",
    "# 修正: y_predを1次元のnumpy配列に変換\n",
    "y_pred_1d = y_pred.reshape(y_pred.shape[0])\n",
    "# 速度の向上: numpyのconcatenate関数を使用\n",
    "comparison_df = pd.DataFrame(np.concatenate((y_test.values.reshape(-1, 1), y_pred_1d.reshape(-1, 1)), axis=1), columns=['Actual Time', 'Predicted Time'])\n",
    "    \n",
    "print(comparison_df.head(10))\n",
    "print(comparison_df.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルで予測(未完成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/tmp/ipykernel_2636/1498754073.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_for_prediction = df_for_prediction.applymap(lambda x: pd.to_numeric(x, errors='coerce') if isinstance(x, str) else x)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_12\" is incompatible with the layer: expected axis -1 of input shape to have value 17, but received input with shape (4, 15)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(4, 15), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 予測\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# モデルの入力形状に合わせて NumPy 配列を調整\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df_numpy_adjusted \u001b[38;5;241m=\u001b[39m df_numpy\u001b[38;5;241m.\u001b[39mreshape(df_numpy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_numpy_adjusted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 予測結果を DataFrame に追加\u001b[39;00m\n\u001b[1;32m     26\u001b[0m df_for_prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape(y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_12\" is incompatible with the layer: expected axis -1 of input shape to have value 17, but received input with shape (4, 15)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(4, 15), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# モデルの読み込み\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "file_path = '../notebook/data-shin/test_data.csv'\n",
    "# CSVファイルの読み込み\n",
    "df = pd.read_csv(file_path)\n",
    "loaded_model = tf.keras.models.load_model('trained_model.h5')\n",
    "\n",
    "# 特徴量の準備\n",
    "df_for_prediction = df.drop(['Horse Name', 'Jockey', 'Trainer', 'Banushi', 'Shoukin', 'Date', 'Track', 'Race Number'], axis=1)\n",
    "\n",
    "# 数値変換（欠損値処理も追加）\n",
    "df_for_prediction = df_for_prediction.applymap(lambda x: pd.to_numeric(x, errors='coerce') if isinstance(x, str) else x)\n",
    "df_for_prediction.fillna(0, inplace=True)  # 欠損値を 0 で埋める（必要に応じて変更）\n",
    "\n",
    "# NumPy 配列に変換\n",
    "df_numpy = df_for_prediction.to_numpy().astype(np.float32)\n",
    "\n",
    "# 予測\n",
    "# モデルの入力形状に合わせて NumPy 配列を調整\n",
    "df_numpy_adjusted = df_numpy.reshape(df_numpy.shape[0], -1)\n",
    "y_pred = loaded_model.predict(df_numpy_adjusted)\n",
    "\n",
    "# 予測結果を DataFrame に追加\n",
    "df_for_prediction['Predicted Time'] = y_pred.reshape(y_pred.shape[0])\n",
    "\n",
    "# 結果の表示\n",
    "print(df_for_prediction.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
