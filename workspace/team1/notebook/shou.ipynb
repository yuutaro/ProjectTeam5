{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦仮で作ったところまで"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m data_b_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/whale217/dev/ProjectTeam5/workspace/team1/data/merged_data/merged_sorted.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# データBのCSVファイルパス\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# CSVファイルからデータを読み込む\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data_a \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_a_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m data_b \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_b_path)\n\u001b[1;32m     10\u001b[0m data_a\u001b[38;5;241m=\u001b[39mdata_a\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# データAとデータBのCSVファイルのパス\n",
    "data_a_path = '/Users/whale217/dev/ProjectTeam5/workspace/team1/data/original_data/race_table/combined.csv'  # データAのCSVファイルパス\n",
    "data_b_path = '/Users/whale217/dev/ProjectTeam5/workspace/team1/data/merged_data/merged_sorted.csv'  # データBのCSVファイルパス\n",
    "\n",
    "# CSVファイルからデータを読み込む\n",
    "data_a = pd.read_csv(data_a_path,low_memory=False)\n",
    "data_b = pd.read_csv(data_b_path)\n",
    "data_a=data_a.dropna()\n",
    "data_b=data_b.dropna()\n",
    "data_a=data_a.drop(columns=['first_horses', 'second_horses', 'third_horses', 'Time', 'Jockey', 'Trainer'])\n",
    "# Code列でデータを結合 (data_bにdata_aの情報を追加)\n",
    "merged_data = pd.merge(data_b, data_a, on='Code', how='left')\n",
    "\n",
    "#結果の確認\n",
    "merged_data.to_csv('../notebook/data-shin/merged_data_final.csv', index=False)\n",
    "# 各列のユニークな値とその出現回数を取得\n",
    "for i in merged_data.columns:\n",
    "    value_counts = merged_data[i].value_counts()  # 各値とそのカウント\n",
    "    print(f\"{i}: {value_counts}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんかパスが奇妙なエラーを吐くので一旦絶対パス使ってます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib is successfully installed!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"matplotlib is successfully installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# データの読み込み（エンコーディングを指定して文字化け防止）\n",
    "file_path = '/Users/whale217/dev/ProjectTeam5/workspace/team1/notebook/data-shin/merged_data_final.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
    "\n",
    "# 1. Sex/Ageを分割して新しい列に追加\n",
    "df[['Sex', 'Age']] = df['Sex/Age'].str.extract(r'([A-Za-z]+)(\\d+)')\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')  # 数値型に変換、変換できないものはNaNに\n",
    "df.drop(columns=['Sex/Age'], inplace=True)  # 元のSex/Age列を削除\n",
    "\n",
    "# 2. カテゴリカルデータをOne-Hot Encoding\n",
    "categorical_columns = ['Sex', 'Jockey', 'Track', 'Weather', 'Condition', 'Trainer', 'Banushi', 'Horse Name']\n",
    "df = pd.get_dummies(df, columns=categorical_columns, dtype=np.uint8)  # メモリ効率の良い型を指定\n",
    "\n",
    "# 3. 欠損値の処理（欠損値を削除、文字列データに対してNaNを取り扱う）\n",
    "df.dropna(subset=['Sex', 'Age'], inplace=True)  # SexやAgeに欠損があれば削除\n",
    "df.dropna(inplace=True)  # その他の欠損値がある行を削除\n",
    "\n",
    "# 4. インデックスの異常データを削除（インデックスが重複している行を削除）\n",
    "df = df.loc[~df.index.duplicated(), :]  # 重複するインデックスを削除\n",
    "\n",
    "# 5. メモリの最適化（データ型の変更）\n",
    "# 数値型のカラムに対して適切な型に変換\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "for col in df.select_dtypes(include=['int64']).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "# 6. 不要な列の削除（CodeやRankなど予測に不要なら削除）\n",
    "columns_to_drop = ['Code', 'Rank', 'Frame Rank']  # 必要に応じて列を指定\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# 7. 前処理後のデータを新しいCSVファイルに保存\n",
    "output_path = 'processed_horse_data_optimized.csv'  # 保存するファイル名を指定\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"前処理が完了しました。処理済みデータは {output_path} に保存されました。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = [\n",
    "    \"Rank\",\"Kinryou\",\"Nobori\",\"Ninki\",\n",
    "    \"Race Number\",\"Distance\",\"Weight\",\"Weight Change\",\n",
    "    \"Sex\",\"Age\",\"Ground_ダ\",\"Ground_芝\",\"Ground_障\",\"Condition_不\",\n",
    "    \"Condition_稍\",\"Condition_良\",\"Condition_重\",\"Weather_小雨\",\"Weather_小雪\",\n",
    "    \"Weather_晴\",\"Weather_曇\",\"Weather_雨\",\"Weather_雪\"\n",
    "]\n",
    "target = \"Time_x\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランクは相対的であると判断し削除。人気は予想がすでに反映されている可能性がある。。。？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/s3g9vbzd3vvch0mp2zlmz2pr0000gn/T/ipykernel_2030/854844578.py:8: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 6.639521235338716\n",
      "Lasso coefficients:\n",
      "[ 1.61775345e-01  2.04326772e-02  1.82504574e-01 -5.86524520e-02\n",
      " -2.39697959e-01  6.84842906e-02 -1.16137627e-02  6.31409587e-04\n",
      "  0.00000000e+00  2.37265676e-01  0.00000000e+00 -6.39200144e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# データの読み込み\n",
    "file_path = '/Users/whale217/dev/ProjectTeam5/workspace/team1/data/format_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 必要な特徴量とターゲット変数の選択\n",
    "X = df[[\n",
    "    \"Kinryou\", \"Nobori\",\"Ninki\",\n",
    "    \"Race Number\", \"Distance\", \"Weight\", \"Weight Change\",\n",
    "    \"Sex\", \"Age\", \"Ground_ダ\", \"Ground_芝\", \"Ground_障\", \n",
    "    \"Condition_不\", \"Condition_稍\", \"Condition_良\", \"Condition_重\", \n",
    "    \"Weather_小雨\", \"Weather_小雪\", \"Weather_晴\", \"Weather_曇\", \"Weather_雨\", \"Weather_雪\"\n",
    "]]\n",
    "\n",
    "y = df['Time_x']  # ターゲット変数\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]  # Xのインデックスに合わせてyも調整\n",
    "\n",
    "# データの分割 (訓練データとテストデータ)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Lasso回帰モデルの作成と学習\n",
    "lasso = Lasso(alpha=0.1)  # alphaは正則化の強さ\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# 予測\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# 評価 (MSEを表示)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Lassoモデルの係数を表示\n",
    "print(\"Lasso coefficients:\")\n",
    "print(lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制度は微妙だが動いてそうだから次に進めます。\n",
    "以下は学習後に実際に予測させてます。（最初にモデルを作ってます。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/s3g9vbzd3vvch0mp2zlmz2pr0000gn/T/ipykernel_2030/81795970.py:9: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 6.703728165706276\n",
      "Lasso coefficients:\n",
      "[ 2.16649614e-02  1.86280339e-01 -4.23405786e-02 -2.40494524e-01\n",
      "  6.84975337e-02 -1.17463394e-02  8.73614315e-04  0.00000000e+00\n",
      "  2.39945628e-01  0.00000000e+00 -6.38672882e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lasso_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# データの読み込み\n",
    "file_path = '/Users/whale217/dev/ProjectTeam5/workspace/team1/data/format_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rankを除外して特徴量とターゲット変数を選択\n",
    "X = df[[\n",
    "    \"Kinryou\", \"Nobori\", \"Ninki\",\n",
    "    \"Race Number\", \"Distance\", \"Weight\", \"Weight Change\",\n",
    "    \"Sex\", \"Age\", \"Ground_ダ\", \"Ground_芝\", \"Ground_障\", \n",
    "    \"Condition_不\", \"Condition_稍\", \"Condition_良\", \"Condition_重\", \n",
    "    \"Weather_小雨\", \"Weather_小雪\", \"Weather_晴\", \"Weather_曇\", \"Weather_雨\", \"Weather_雪\"\n",
    "]]\n",
    "\n",
    "y = df['Time_x']  # ターゲット変数\n",
    "\n",
    "# 欠損値を含む行を削除\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]  # Xのインデックスに合わせてyも調整\n",
    "\n",
    "# データの分割 (訓練データとテストデータ)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Lasso回帰モデルの作成と学習\n",
    "lasso = Lasso(alpha=0.1)  # alphaは正則化の強さ\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# 予測\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# 評価 (MSEを表示)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Lassoモデルの係数を表示\n",
    "print(\"Lasso coefficients:\")\n",
    "print(lasso.coef_)\n",
    "\n",
    "#モデルの保持\n",
    "model_filename = 'lasso_model.pkl'\n",
    "joblib.dump(lasso, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測されたTime_x: 117.39123526056599\n"
     ]
    }
   ],
   "source": [
    "# モデルの読み込み\n",
    "loaded_model = joblib.load('lasso_model.pkl')\n",
    "\n",
    "# 新しいデータに対して予測を行う\n",
    "new_data = pd.DataFrame({\n",
    "    \"Kinryou\": [55.0],\n",
    "    \"Nobori\": [35.2],\n",
    "    \"Ninki\": [3],\n",
    "    \"Race Number\": [5],\n",
    "    \"Distance\": [1800],\n",
    "    \"Weight\": [470],\n",
    "    \"Weight Change\": [2],\n",
    "    \"Sex\": [1],\n",
    "    \"Age\": [4],\n",
    "    \"Ground_ダ\": [1],\n",
    "    \"Ground_芝\": [0],\n",
    "    \"Ground_障\": [0],\n",
    "    \"Condition_不\": [0], \"Condition_稍\": [1], \"Condition_良\": [0], \"Condition_重\": [0],\n",
    "    \"Weather_小雨\": [0], \"Weather_小雪\": [0], \"Weather_晴\": [1], \"Weather_曇\": [0],\n",
    "    \"Weather_雨\": [0], \"Weather_雪\": [0]\n",
    "})\n",
    "\n",
    "# モデルで新しいデータを予測\n",
    "predicted_time = loaded_model.predict(new_data)\n",
    "\n",
    "# 予測結果を表示\n",
    "print(f\"予測されたTime_x: {predicted_time[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
